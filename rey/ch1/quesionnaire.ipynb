{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1\n",
    "\n",
    "1. Can do deep learning without lots of math, lots of data, lots of expensive compute, or a PhD\n",
    "2. Deep learning is currently best in the world at NLP (incl document summarization, classification), image classification, image segmentation, speech recognition, text to speech, text to image, NER\n",
    "3. The first device that was based on the principle of the artificial neuron was [Mark I] perceptron [by Frank Rosenblatt]\n",
    "4. > A set of processing units, a state of activation, an output function for each unit, a pattern of connectivity among units, a propagation rule for propagating patterns of activities through the network of connectivities, an activation rule for combining the inputs impinging on a unit with the current state of that unit to produce an output for that unit, a learning rule whereby patterns of connectivity are modified by experience, an environment within which the system must operate.\n",
    "5. The first theoretical misunderstanding that held back the field of neural nets was that ~complex operations~ simple but critical mathematical functions (such as XOR) were not able to be identified by just a one layer network/device. The other is that networks with 2 layers of neurons were originally (80s, 90s) too slow to be useful.\n",
    "6. GPU stands for Graphics Processing Unit\n",
    "7. 1+1 = 2\n",
    "8. Done\n",
    "9. Done\n",
    "10. It's hard to use a traditional program to recognize images in a photo because humans perform this task effortlessly so it is hard for them to enumerate all of the requisite features. Furthermore, it would be tedious to program all of these features using traditional code.\n",
    "11. Weight assignment has to do with iteratively tweaking values on the path toward reducing error rate. > In other words, evaluation and training of the model in order to obtain a set of parameter values that maximize model performance. As a noun, weight assignment refers to the current values of the model parameters. \n",
    "12. Samuel's weights are normally called parameters\n",
    "13. input,weights -> model -> result -> performance -> weights\n",
    "14. It's hard to understand why a deep learning model makes a particular predition because the internals of the neural net aren't explicitly visible. Also, there are many more parameters than can be analyzed at a glance. \n",
    "15. > Universal approximation theorem\n",
    "16. In order to train a model you need accurately labeled data. > You need an architecture for the given problem, labeled data, a loss function that will quantitatively measure the performance of the model, a way to update the parameters of the model in order to improve its performance.\n",
    "17. One of the issues with a predictive policing model is that biases that were originally present may be amplified. Police may use the model to determine where to focus activity, leading to more arrests in those areas, which would be fed back into the model, leading to an even more biased model. This would lead to a highly biased model with little predictive power. \n",
    "18. 224x224 was a requisite size for an old model. With modern models we can use any size as long as all of the input images get resized to the same dimensions. > You can increase the size and get better performance at the price of speed and memory consumption.\n",
    "19. Classification predicts a label/class/category from a discrete set. Regression is predicting a numeric quantity.\n",
    "20. Labeled data is segmented into 3 discrete parts - training set, validation set, and test set. Training set is used for training the model. Validation set is used for verifying the model. Test set is also used for verification of model correctness but is kept hidden from the engineer. \n",
    "> The validation set is the portion of the dataset that is not used for training the model, but for evaluating the model during training, in order to prevent overfitting. This ensures that the model performance is not due to “cheating” or memorization of the dataset, but rather because it learns the appropriate features to use for prediction. However, it is possible that we overfit the validation data as well. This is because the human modeler is also part of the training process, adjusting hyperparameters (see question 32 for definition) and training procedures according to the validation performance. Therefore, another unseen portion of the dataset, the test set, is used for final evaluation of the model. This splitting of the dataset is necessary to ensure that the model generalizes to unseen data.\n",
    "21. If you don't provide a validation set then fastai will segment data for you 0.8/0.2.\n",
    "22. You can't always use a random sample for the validation set due to the nature of the data. For timeseries data, a random selection would make it too easy for the model to fill in the gaps and overfit - better to select different time periods for validation, training, and test sets. For other kinds of data it could also vary, for example if labeling person behavior then data should be segmented by person such that no single person's data exists in more than one set (training, validation, test).\n",
    "> A good validation or test set should be representative of new data you will see in the future.\n",
    "23. Overfitting happens when the model is training poorly such that it \"memorizes\" specific features of the training set instead of learning general rules. This causes the model to perform poorly on new, unseen, data.\n",
    "> When the model fits too closely to a limited set of data but does not generalize well to new data.\n",
    "24. A metric is a human-readable and human-useful function of how well the model performs on the validation set. Loss is also a function of how well the model performs but it is used by optimization algorithm (SGD) to update model parameters.\n",
    "25. Pretrained models allow us to get results quicker and cheaper.\n",
    "26. When using a pretrained model, the ~head is~ later layers, which were useful for the task that the model was originally trained on, are removed and replaced by new layers with randomized weights. These new layers are called the \"head\".\n",
    "27. The early layers of a CNN find the smallest visual features such as straight lines or gradients. The latest layers find more complex features such as wheels, faces, text.\n",
    "28. Image models are useful for any data that can be transformed into meaningful images, including but not limited to sound (spetrograms), mouse movement \"maps\", timeseries data.\n",
    "29. Architecture is the template or structure of a model. It does not involve any data parameter values but defines the mathematical structure of how they will be used. Architecture + data => model\n",
    "30. Segmentation is a process for labeling each pixel in an image.\n",
    "31. > defines the range of (continuous) values, used for collaborative filtering (also, perhaps, other contexts)\n",
    "32. Hyperparameters are meta parameters that the engineer has direct control over - architecture, pretrained model selection, data massaging, how long to train for, learning rate. Parameters refer to the weights that are part of the model.\n",
    "33. Be mindful of biases included in data, they may be exacerbated when using the model. Be mindful of overfitting. \n",
    "> Make sure training, validation, and test sets are defined properly. \n",
    "\n",
    "## Further research\n",
    "1. A GPU is more useful for deep learning than a CPU because it has more memory, more memory bandwidth, and more parallel processing units. This enables GPUs to outperform CPUs when doing matrix multiplication, which causes them to be better for training NNs.\n",
    "2. *Think of ares where feedback loops might impact the use of machine learning*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
