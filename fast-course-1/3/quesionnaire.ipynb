{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4\n",
    "\n",
    "1. How is a grayscale image represented on a computer? How about a color image?\n",
    "   > Grayscale images are represented by an matrix. Color images are represented by 3 or 4 layers of matrices.\n",
    "2. How are the files and folders in the MNIST_SAMPLE dataset structured? Why?\n",
    "   > Training data is separated from validation data, as per best practice. Further down, there is a folder for each category.\n",
    "3. Explain how the \"pixel similarity\" approach to classifying digits works.\n",
    "   > This is a baseline approach. First, \"ideal\" versions of the images are calculated. This is one image for each digit, with each pixel having the average value for the digits in the dataset.\n",
    "4. What is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\n",
    "   > [num*2 for num in nums if num % 2 == 1]\n",
    "5. What is a \"rank-3 tensor\"?\n",
    "   > a cuboid\n",
    "6. What is the difference between tensor rank and shape? How do you get the rank from the shape?\n",
    "   > len(shape) == rank\n",
    "   > shape is the size of each axis of a tensor\n",
    "7. What are RMSE and L1 norm?\n",
    "   L1 norm / MAE (mean absolute error) - (x-y).abs().mean()\n",
    "   L2 norm / RMSE - sqrt(((x-y)^2).mean())\n",
    "8. How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?\n",
    "   Using pytorch tensors! It runs in C / CUDA\n",
    "9.  Create a 3Ã—3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.\n",
    "    t = tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "    t *= 2\n",
    "    t[1:3,1:3]\n",
    "    also\n",
    "    t = tensor(list(range(1,10))).view(3,3)\n",
    "    t *= 2\n",
    "    t[1:, 1:]\n",
    "10. What is broadcasting?\n",
    "    Performing an operation on two tensors, where one is lower rank than the other. Expanding the lower rank tensor (copying or pretending to copy) such that the tensors have the same size and the operation can be performed.\n",
    "11. Are metrics generally calculated using the training set, or the validation set? Why?\n",
    "    > validation set because the model hasn't seen the validation set\n",
    "12. What is SGD?\n",
    "    > Stochastic gradient descent. An approach for maneuvering through the loss space (toward the minimum) by adjusting the parameters.\n",
    "    > An optimization algorithm that will update the parameters of a model in order to minimize a given loss function that was evaluated on the predictions and target.\n",
    "13. Why does SGD use mini-batches?\n",
    "    > often it isn't feasible to compute SGD on the whole dataset, as the GPU would run out of memory. Stepping through each item one at a time would not work either as the gradient would be unstable and imprecise. As a compromise, we calculate the avg loss for a small subset of the dataset (called a mini-batch) at a time. This is also more computationally efficient, than single items, on a GPU!\n",
    "14. What are the seven steps in SGD for machine learning?\n",
    "    1. initialize parameters - random values work best\n",
    "    2. predict, on training set - one mini-batch at a time\n",
    "    3. compute loss - avg loss over the minibatch is calculated\n",
    "    4. calculate gradient - a direction of how the params need to change to minimize loss\n",
    "    5. step / adjust parameters\n",
    "    6. repeat (go to step 2)\n",
    "    7. done\n",
    "15. How do we initialize the weights in a model?\n",
    "    > to random values\n",
    "16. What is \"loss\"?\n",
    "    > a value indicating the performance of the model, used to calculate gradient\n",
    "17. Why can't we always use a high learning rate?\n",
    "    > it may shoot the loss higher such that the alg may not be able to find the minimum\n",
    "18. What is a \"gradient\"?\n",
    "    > slope of the loss with respect to parameters\n",
    "19. Do you need to know how to calculate gradients yourself?\n",
    "    > no, pytorch does it for us\n",
    "20. Why can't we use accuracy as a loss function?\n",
    "    > it doesn't have the right shape\n",
    "21. Draw the sigmoid function. What is special about its shape?\n",
    "    > it looks like an S. Y is bound between 0 and 1\n",
    "22. What is the difference between a loss function and a metric?\n",
    "    > loss function is used for training the model\n",
    "    > metric is used for human understanding and judging of the model\n",
    "23. What is the function to calculate new weights using a learning rate?\n",
    "    > with no gradient: params -= params.grad * lr\n",
    "    > the optimizer step function\n",
    "24. What does the DataLoader class do?\n",
    "    > shuffling and mini-batch collation\n",
    "25. Write pseudocode showing the basic steps taken in each epoch for SGD.\n",
    "    > def step(xb, yb):\n",
    "    >   preds = model.predict(xb, params)\n",
    "    >   loss = mse(preds, yb)\n",
    "    >   loss.backward()\n",
    "    >   with no gradient:\n",
    "    >     params -= params.grad * lr\n",
    "26. Create a function that, if passed two arguments [1,2,3,4] and 'abcd', returns [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]. What is special about that output data structure?\n",
    "    > def foo(x,y): return list(zip(x,y))\n",
    "27. What does view do in PyTorch?\n",
    "    > reshapes the tensor, changing its dimensions\n",
    "28. What are the \"bias\" parameters in a neural network? Why do we need them?\n",
    "    > bias are the b in y=mx+b. They are the constants in the linear equation.\n",
    "    > without bias params, if the input is 0 then the output will be 0. Therefore, using bias parameters adds additional flexibility to the model.\n",
    "29. What does the @ operator do in Python?\n",
    "    > matrix multiplication\n",
    "30. What does the backward method do?\n",
    "    > calculates gradient\n",
    "31. Why do we have to zero the gradients?\n",
    "    > because otherwise they get added to previously calculated gradients!\n",
    "32. What information do we have to pass to Learner?\n",
    "    > dataloaders, model, optimization fn, loss fn. Optionally, metrics to print\n",
    "33. Show Python or pseudocode for the basic steps of a training loop.\n",
    "    > def train_epoch(model, lr, params):\n",
    "    >   for xb,yb in dl:\n",
    "    >       calc_grad(xb, yb, model)\n",
    "    >       for p in params:\n",
    "    >           with torch.no_grad():\n",
    "    >               p -= p.grad * lr\n",
    "    >           p.grad.zero_()\n",
    "    > for i in range(20):\n",
    "    >   train_epoch(model, lr, params)\n",
    "34. What is \"ReLU\"? Draw a plot of it for values from -2 to +2.\n",
    "    > horizontal line at y=0 when x<=0 >, normal slope when x>0\n",
    "35. What is an \"activation function\"?\n",
    "    > ReLU aka nonlinearity\n",
    "36. What's the difference between F.relu and nn.ReLU?\n",
    "    > F.relu is a function\n",
    "    > nn.ReLU is a class\n",
    "37. The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?\n",
    "    > By using deeper NNs, we can achieve better results with less parameters. We can train the model quicker and it will use less memory!\n",
    "    > We can use a deeper model with less number of parameters, better performance, faster training, and less compute/memory requirements.\n",
    "\n",
    "# Further Research\n",
    "1. Create your own implementation of Learner from scratch, based on the training loop shown in this chapter.\n",
    "2. Complete all the steps in this chapter using the full MNIST datasets (that is, for all digits, not just 3s and 7s). This is a significant project and will take you quite a bit of time to complete! You'll need to do some of your own research to figure out how to overcome some obstacles you'll meet on the way."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
